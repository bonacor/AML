{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve Performance with Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensembles** are **methods combining the predictions from different models**. They can give you a boost in accuracy on your dataset, so this approach allows the production of better predictive performance compared to a single model (and that is why ensemble methods often place first in many prestigious ML competitions, such as the Netflix Competition and Kaggle).\n",
    "\n",
    "Now you will discover how you can create some of the most powerful types of ensembles in Python using scikit-learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Models Into Ensemble Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three most popular methods for combining the predictions from different models are:\n",
    "\n",
    "* ***Bagging***. Building multiple models (typically of the same type) **from different subsamples of the training dataset**.\n",
    "* ***Boosting***. Building multiple models (typically of the same type) **each of which learns to fix the prediction errors of a prior model in the sequence of models**.\n",
    "* ***Voting***. Building multiple models (typically of differing types) and simple statistics (like calculating the mean) are used to combine predictions.\n",
    "\n",
    "*DISCLAIMER: The following assumes you are generally familiar with ML algorithms and ensemble methods and/or may want to ignore the details for the time being, and this part will not go into the details of how the algorithms work or their parameters.*\n",
    "\n",
    "We will use the Pima indians diabetes dataset to demonstrate each algorithm. Each ensemble algorithm is demonstrated using 10-fold cross-validation and the classification accuracy performance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap Aggregation (or Bagging) involves taking multiple samples from your training dataset (with replacement) and training a model for each sample. The final output prediction is averaged across the predictions of all of the sub-models. \n",
    "\n",
    "The three bagging models we will cover here are as follows:\n",
    "* Bagged Decision Trees\n",
    "* Random Forest\n",
    "* Extra Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagged Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging performs best with algorithms that have **high variance**. \n",
    "\n",
    "In the example below is an example of using the $BaggingClassifier$ with the Classification and Regression Trees algorithm ($DecisionTreeClassifier$ - more info [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)). A total of 100 trees are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier                     # <---\n",
    "from sklearn.tree import DecisionTreeClassifier                    # <---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.770745044429\n"
     ]
    }
   ],
   "source": [
    "# Bagged Decision Trees for Classification\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(NOTE: running the cell above should take some more time than usual..)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests is **an extension of bagged decision trees**. \n",
    "\n",
    "You can construct a Random Forest model for classification using the RandomForestClassifier class, documented [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "\n",
    "The example below demonstrates using Random Forest for classification with 100 trees and split points chosen from a random selection of 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier                    # <---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.762987012987\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classification\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(NOTE: running the cell above should take some more time than usual..)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Trees are **another modification of bagging** where random trees are constructed from samples of the training dataset. \n",
    "\n",
    "You can construct an Extra Trees model for classification using the $ExtraTreesClassifier$ class (documented [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)). \n",
    "\n",
    "The example below provides a demonstration of extra trees with the number of trees set to 100 and splits chosen from 7 random features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier                    # <---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.762935748462\n"
     ]
    }
   ],
   "source": [
    "# Extra Trees Classification\n",
    "num_trees = 100\n",
    "max_features = 7\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(NOTE: running the cell above should take some more time than usual..)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting ensemble algorithms **create a sequence of models that attempt to correct the mistakes of the models before them in the sequence**. \n",
    "\n",
    "The 2 most common boosting ensemble ML algos are:\n",
    "* AdaBoost\n",
    "* Stochastic Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can construct an AdaBoost model for classification using the $AdaBoostClassifier$ class (documented [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)).\n",
    "\n",
    "The example below demonstrates the construction of 30 decision trees in sequence using the AdaBoost algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier                    # <---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76045796309\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost Classification\n",
    "num_trees = 30\n",
    "seed=7\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can construct a Gradient Boosting model for classification using the $GradientBoostingClassifier$ class (documented [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble. GradientBoostingClassifier.html)). \n",
    "\n",
    "The example below demonstrates Stochastic Gradient Boosting for classification with 100 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier                    # <---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.766900205058\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Boosting Classification\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example provides a mean estimate of classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a voting ensemble model for classification using the $VotingClassifier$ class (documented [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)). \n",
    "\n",
    "The code below provides an example of combining the predictions of logistic regression, classification and regression trees and support vector machines together for a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier                    # <---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73038277512\n"
     ]
    }
   ],
   "source": [
    "# Voting Ensemble for Classification\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "\n",
    "# create the sub models\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(( 'logistic' , model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(( 'cart' , model2))\n",
    "model3 = SVC()\n",
    "estimators.append(( 'svm' , model3))\n",
    "\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did:\n",
    "\n",
    "* we discovered ensemble ML algorithms for improving the performance of models on your problems. You learned about\n",
    "Bagging Ensembles including Bagged Decision Trees, Random Forest and Extra Trees, Boosting Ensembles including AdaBoost and Stochastic Gradient Boosting, Voting Ensembles for averaging the predictions for any arbitrary models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will discover algorithm tuning, another technique that you can use to improve the performance of algorithms on your dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
