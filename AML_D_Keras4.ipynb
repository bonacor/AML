{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson you will discover how you can use deep learning models from Keras with the scikit-learn library in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# MLP for Pima Indians Dataset with 10-fold cross validation via sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10, verbose=0)\n",
    "# evaluate using 10-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Deep Learning Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.761719 using {'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 5}\n",
      "0.678385 (0.022628) with: {'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'batch_size': 5}\n",
      "0.673177 (0.043303) with: {'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam', 'batch_size': 5}\n",
      "0.709635 (0.016367) with: {'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop', 'batch_size': 5}\n",
      "0.707031 (0.012758) with: {'epochs': 50, 'init': 'normal', 'optimizer': 'adam', 'batch_size': 5}\n",
      "0.692708 (0.016367) with: {'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 5}\n",
      "0.696615 (0.031948) with: {'epochs': 50, 'init': 'uniform', 'optimizer': 'adam', 'batch_size': 5}\n",
      "0.697917 (0.017566) with: {'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'batch_size': 5}\n",
      "0.671875 (0.041707) with: {'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam', 'batch_size': 5}\n",
      "0.735677 (0.016367) with: {'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop', 'batch_size': 5}\n",
      "0.721354 (0.012075) with: {'epochs': 100, 'init': 'normal', 'optimizer': 'adam', 'batch_size': 5}\n",
      "0.727865 (0.018136) with: {'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 5}\n",
      "0.750000 (0.033603) with: {'epochs': 100, 'init': 'uniform', 'optimizer': 'adam', 'batch_size': 5}\n",
      "0.699219 (0.041463) with: {'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'batch_size': 5}\n",
      "0.707031 (0.043848) with: {'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam', 'batch_size': 5}\n",
      "0.723958 (0.008027) with: {'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop', 'batch_size': 5}\n",
      "0.740885 (0.017566) with: {'epochs': 150, 'init': 'normal', 'optimizer': 'adam', 'batch_size': 5}\n",
      "0.761719 (0.013902) with: {'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 5}\n",
      "0.751302 (0.027498) with: {'epochs': 150, 'init': 'uniform', 'optimizer': 'adam', 'batch_size': 5}\n",
      "0.673177 (0.045256) with: {'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'batch_size': 10}\n",
      "0.654948 (0.021236) with: {'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam', 'batch_size': 10}\n",
      "0.686198 (0.012890) with: {'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop', 'batch_size': 10}\n",
      "0.700521 (0.020752) with: {'epochs': 50, 'init': 'normal', 'optimizer': 'adam', 'batch_size': 10}\n",
      "0.686198 (0.014382) with: {'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 10}\n",
      "0.695313 (0.005524) with: {'epochs': 50, 'init': 'uniform', 'optimizer': 'adam', 'batch_size': 10}\n",
      "0.678385 (0.030145) with: {'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'batch_size': 10}\n",
      "0.558594 (0.167681) with: {'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam', 'batch_size': 10}\n",
      "0.692708 (0.035132) with: {'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop', 'batch_size': 10}\n",
      "0.731771 (0.018688) with: {'epochs': 100, 'init': 'normal', 'optimizer': 'adam', 'batch_size': 10}\n",
      "0.722656 (0.019401) with: {'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 10}\n",
      "0.705729 (0.010253) with: {'epochs': 100, 'init': 'uniform', 'optimizer': 'adam', 'batch_size': 10}\n",
      "0.697917 (0.008027) with: {'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'batch_size': 10}\n",
      "0.701823 (0.020752) with: {'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam', 'batch_size': 10}\n",
      "0.739583 (0.019225) with: {'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop', 'batch_size': 10}\n",
      "0.709635 (0.015073) with: {'epochs': 150, 'init': 'normal', 'optimizer': 'adam', 'batch_size': 10}\n",
      "0.736979 (0.019488) with: {'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 10}\n",
      "0.727865 (0.021236) with: {'epochs': 150, 'init': 'uniform', 'optimizer': 'adam', 'batch_size': 10}\n",
      "0.593750 (0.149462) with: {'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'batch_size': 20}\n",
      "0.660156 (0.028705) with: {'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam', 'batch_size': 20}\n",
      "0.669271 (0.023073) with: {'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop', 'batch_size': 20}\n",
      "0.692708 (0.025780) with: {'epochs': 50, 'init': 'normal', 'optimizer': 'adam', 'batch_size': 20}\n",
      "0.697917 (0.011201) with: {'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 20}\n",
      "0.690104 (0.011201) with: {'epochs': 50, 'init': 'uniform', 'optimizer': 'adam', 'batch_size': 20}\n",
      "0.658854 (0.037240) with: {'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'batch_size': 20}\n",
      "0.682292 (0.017566) with: {'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam', 'batch_size': 20}\n",
      "0.707031 (0.006379) with: {'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop', 'batch_size': 20}\n",
      "0.713542 (0.017566) with: {'epochs': 100, 'init': 'normal', 'optimizer': 'adam', 'batch_size': 20}\n",
      "0.713542 (0.013279) with: {'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 20}\n",
      "0.709635 (0.020505) with: {'epochs': 100, 'init': 'uniform', 'optimizer': 'adam', 'batch_size': 20}\n",
      "0.680990 (0.021710) with: {'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'batch_size': 20}\n",
      "0.716146 (0.018136) with: {'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam', 'batch_size': 20}\n",
      "0.721354 (0.013279) with: {'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop', 'batch_size': 20}\n",
      "0.718750 (0.017758) with: {'epochs': 150, 'init': 'normal', 'optimizer': 'adam', 'batch_size': 20}\n",
      "0.708333 (0.019488) with: {'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 20}\n",
      "0.723958 (0.012075) with: {'epochs': 150, 'init': 'uniform', 'optimizer': 'adam', 'batch_size': 20}\n"
     ]
    }
   ],
   "source": [
    "# MLP for Pima Indians Dataset with grid search via sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(8, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "inits = ['glorot_uniform', 'normal', 'uniform']\n",
    "epochs = [50, 100, 150]\n",
    "batches = [5, 10, 20]\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=inits)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might take about 5 minutes to complete on your workstation executed on the CPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we learned:\n",
    "\n",
    "* we discovered how you can wrap your Keras deep learning models and use them in the scikit-learn general machine learning library. \n",
    "\n",
    "Specifically:\n",
    "\n",
    "* how to wrap Keras models so that they can be used with the scikit-learn machine learning library.\n",
    "* How to use a wrapped Keras model as part of evaluating model performance in scikit-learn.\n",
    "* How to perform hyperparameter tuning in scikit-learn using a wrapped Keras model.\n",
    "\n",
    "You can see that using scikit-learn for standard machine learning operations such as model evaluation and model hyperparameter optimization can save a lot of time over implementing these schemes yourself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have fun with Keras and DL!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
